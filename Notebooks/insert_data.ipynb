{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Fixed Database Insertion Script - All Errors Resolved\n",
    "\n",
    "This notebook properly handles:\n",
    "- Phone number truncation\n",
    "- Foreign key relationships (by mapping IDs back from the DB)\n",
    "- Column mapping issues\n",
    "- Identity column handling and table cleanup for repeatable loads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4",
   "metadata": {},
   "source": [
    "## Setup and Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "setup_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import numpy as np\n",
    "import warnings\n",
    "import traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database connection\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://localhost\\\\SQLEXPRESS/Book_haven?\"\n",
    "    \"driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connected successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test connection\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"SELECT 1\"))\n",
    "    print(\"Database connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_header",
   "metadata": {},
   "source": [
    "## PRE-RUN CLEANUP: Clear All Tables and Reset Identity\n",
    "\n",
    "This is essential to avoid Primary Key and Unique Constraint violations on re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db_cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database tables cleared and Identity counters reset.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with engine.begin() as conn:\n",
    "        # Delete children first due to Foreign Key constraints\n",
    "        conn.execute(text(\"DELETE FROM reservation_details\"))\n",
    "        conn.execute(text(\"DELETE FROM reservation\"))\n",
    "        conn.execute(text(\"DELETE FROM book_author\"))\n",
    "        conn.execute(text(\"DELETE FROM book_category\"))\n",
    "        conn.execute(text(\"DELETE FROM book_copy\"))\n",
    "        conn.execute(text(\"DELETE FROM book\"))\n",
    "        conn.execute(text(\"DELETE FROM staff\"))\n",
    "        conn.execute(text(\"DELETE FROM member\"))\n",
    "        conn.execute(text(\"DELETE FROM category\"))\n",
    "        conn.execute(text(\"DELETE FROM author\"))\n",
    "        conn.execute(text(\"DELETE FROM description\"))\n",
    "        \n",
    "        # Reset Identity/Auto-increment counters for all tables\n",
    "        conn.execute(text(\"DBCC CHECKIDENT ('description', RESEED, 0)\"))\n",
    "        conn.execute(text(\"DBCC CHECKIDENT ('author', RESEED, 0)\"))\n",
    "        conn.execute(text(\"DBCC CHECKIDENT ('category', RESEED, 0)\"))\n",
    "        conn.execute(text(\"DBCC CHECKIDENT ('member', RESEED, 0)\"))\n",
    "        conn.execute(text(\"DBCC CHECKIDENT ('staff', RESEED, 0)\"))\n",
    "        conn.execute(text(\"DBCC CHECKIDENT ('book_copy', RESEED, 0)\"))\n",
    "        conn.execute(text(\"DBCC CHECKIDENT ('reservation', RESEED, 0)\"))\n",
    "        # reservation_details usually doesn't have an identity column\n",
    "        \n",
    "        print(\"Database tables cleared and Identity counters reset.\")\n",
    "except Exception as e:\n",
    "    print(f\"Cleanup failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1_header",
   "metadata": {},
   "source": [
    "## Step 1: Insert Independent Tables (No Foreign Keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc_header",
   "metadata": {},
   "source": [
    "### 1.1 Description Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 2500 descriptions\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    description = pd.read_csv(\"data/description.csv\")\n",
    "    description = description[['description']].copy()\n",
    "    description.dropna(subset=['description'], inplace=True)\n",
    "    description.to_sql('description', con=engine, if_exists='append', index=False)\n",
    "    print(f\"Inserted {len(description)} descriptions\")\n",
    "except Exception as e:\n",
    "    print(f\"Description insertion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "author_header",
   "metadata": {},
   "source": [
    "### 1.2 Author Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_author",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 4460 authors\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    author = pd.read_csv(\"data/author.csv\")\n",
    "    author = author[['name']].copy()\n",
    "    author['name'] = author['name'].str.strip()\n",
    "    author.dropna(subset=['name'], inplace=True)\n",
    "    author.drop_duplicates(subset=['name'], inplace=True)\n",
    "    author.to_sql('author', con=engine, if_exists='append', index=False)\n",
    "    print(f\" Inserted {len(author)} authors\")\n",
    "except Exception as e:\n",
    "    print(f\" Author insertion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "category_header",
   "metadata": {},
   "source": [
    "### 1.3 Category Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "insert_category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 802 categories\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    category = pd.read_csv(\"data/category.csv\")\n",
    "    category = category[['category_name']].copy()\n",
    "    category['category_name'] = category['category_name'].str.strip()\n",
    "    category.dropna(subset=['category_name'], inplace=True)\n",
    "    category.drop_duplicates(subset=['category_name'], inplace=True)\n",
    "    category.to_sql('category', con=engine, if_exists='append', index=False)\n",
    "    print(f\"✅ Inserted {len(category)} categories\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Category insertion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "member_header",
   "metadata": {},
   "source": [
    "### 1.4 Member Table - WITH PHONE TRUNCATION FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 3000 members\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    member = pd.read_csv(\"data/member.csv\")\n",
    "    member_cols = ['fname', 'lname', 'phone', 'email', 'city', 'street', 'bdate']\n",
    "    member = member[member_cols].copy()\n",
    "    \n",
    "    # FIX: Truncate phone numbers to 20 characters (matching SQL schema)\n",
    "    member['phone'] = member['phone'].astype(str).str[:20]\n",
    "    \n",
    "    member.dropna(subset=['fname', 'lname', 'email'], inplace=True)\n",
    "    member.drop_duplicates(subset=['email'], inplace=True)\n",
    "    member.to_sql('member', con=engine, if_exists='append', index=False)\n",
    "    print(f\"Inserted {len(member)} members\")\n",
    "except Exception as e:\n",
    "    print(f\"Member insertion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "staff_header",
   "metadata": {},
   "source": [
    "### 1.5 Staff Table - WITH PHONE TRUNCATION FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 100 staff members\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    staff = pd.read_csv(\"data/staff.csv\")\n",
    "    staff_cols = ['fname', 'lname', 'phone', 'email', 'role', 'password', 'salary']\n",
    "    staff = staff[staff_cols].copy()\n",
    "    \n",
    "    # FIX: Truncate phone numbers to 20 characters (matching SQL schema)\n",
    "    staff['phone'] = staff['phone'].astype(str).str[:20]\n",
    "    \n",
    "    staff.dropna(subset=['fname', 'lname', 'email', 'password'], inplace=True)\n",
    "    staff.drop_duplicates(subset=['email'], inplace=True)\n",
    "    staff.to_sql('staff', con=engine, if_exists='append', index=False)\n",
    "    print(f\"Inserted {len(staff)} staff members\")\n",
    "except Exception as e:\n",
    "    print(f\"Staff insertion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_header",
   "metadata": {},
   "source": [
    "## Step 2: Retrieve Auto-Generated IDs (The FK Mapping Step)\n",
    "\n",
    "This is the critical step! We need to query the actual Primary Keys generated by SQL Server to correctly map Foreign Keys in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieve_ids",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 2500 description IDs\n",
      "✅ Retrieved 4460 author IDs\n",
      "✅ Retrieved 802 category IDs\n",
      "✅ Retrieved 3000 member IDs\n",
      "✅ Retrieved 100 staff IDs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get description IDs\n",
    "    with engine.connect() as conn:\n",
    "        desc_map = pd.read_sql(\"SELECT description_id, description FROM description\", conn)\n",
    "    print(f\"Retrieved {len(desc_map)} description IDs\")\n",
    "    \n",
    "    # Get author IDs\n",
    "    with engine.connect() as conn:\n",
    "        author_map = pd.read_sql(\"SELECT author_id, name FROM author\", conn)\n",
    "    print(f\"Retrieved {len(author_map)} author IDs\")\n",
    "    \n",
    "    # Get category IDs\n",
    "    with engine.connect() as conn:\n",
    "        category_map = pd.read_sql(\"SELECT category_id, category_name FROM category\", conn)\n",
    "    print(f\"Retrieved {len(category_map)} category IDs\")\n",
    "    \n",
    "    # Get member IDs\n",
    "    with engine.connect() as conn:\n",
    "        member_map = pd.read_sql(\"SELECT member_id, email FROM member\", conn)\n",
    "    print(f\"Retrieved {len(member_map)} member IDs\")\n",
    "    \n",
    "    # Get staff IDs\n",
    "    with engine.connect() as conn:\n",
    "        staff_map = pd.read_sql(\"SELECT staff_id, email FROM staff\", conn)\n",
    "    print(f\"Retrieved {len(staff_map)} staff IDs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to retrieve IDs: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_header",
   "metadata": {},
   "source": [
    "## Step 3: Insert Book Table (FK to description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_book",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 2500 books\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    book = pd.read_csv(\"data/book.csv\")\n",
    "    book_clean = book[['ISBN', 'title', 'publication_year']].copy()\n",
    "    \n",
    "    # ASSUMPTION: description IDs match the insertion order of the book rows in the CSV.\n",
    "    if len(book_clean) <= len(desc_map):\n",
    "        book_clean['description_id'] = desc_map['description_id'].iloc[:len(book_clean)].values\n",
    "    else:\n",
    "        print(\"Warning: More books than descriptions! Using only available IDs.\")\n",
    "        book_clean = book_clean.iloc[:len(desc_map)]\n",
    "        book_clean['description_id'] = desc_map['description_id'].values\n",
    "    \n",
    "    # Clean and convert year column\n",
    "    book_clean['publication_year'] = pd.to_numeric(\n",
    "        book_clean['publication_year'], errors='coerce'\n",
    "    ).astype('Int64')\n",
    "    \n",
    "    book_clean.dropna(subset=['ISBN', 'title'], inplace=True)\n",
    "    book_clean.drop_duplicates(subset=['ISBN'], inplace=True)\n",
    "    \n",
    "    book_clean.to_sql('book', con=engine, if_exists='append', index=False)\n",
    "    print(f\"Inserted {len(book_clean)} books\")\n",
    "    \n",
    "    # Retrieve existing ISBNs for subsequent FK checks\n",
    "    with engine.connect() as conn:\n",
    "        existing_isbns = pd.read_sql(\"SELECT ISBN FROM book\", conn)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Book insertion failed: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_header",
   "metadata": {},
   "source": [
    "## Step 4: Insert Junction Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "book_author_header",
   "metadata": {},
   "source": [
    "### 4.1 Book_Author Table - FIXED COLUMN MAPPING (FK to book and author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_book_author",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 6450 book-author relationships\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "try:\n",
    "    book_author = pd.read_csv(\"data/book_author.csv\")\n",
    "    \n",
    "    # 1. Determine the source column name\n",
    "    if 'Authors_list' in book_author.columns:\n",
    "        author_col = 'Authors_list'\n",
    "    elif 'author_id' in book_author.columns: \n",
    "        author_csv = pd.read_csv(\"data/author.csv\")\n",
    "        old_author_map = author_csv[['author_id', 'name']].drop_duplicates()\n",
    "        book_author = book_author.merge(\n",
    "            old_author_map,\n",
    "            on='author_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        author_col = 'name'\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot find author mapping column in book_author CSV\")\n",
    "    \n",
    "    # Pre-merge cleaning\n",
    "    book_author[author_col] = book_author[author_col].astype(str).str.strip()\n",
    "    \n",
    "    # Ensure the author_map is also perfectly clean\n",
    "    author_map['name'] = author_map['name'].astype(str).str.strip()\n",
    "    \n",
    "    # Perform the INNER join to map to the database author_id\n",
    "    # No need for diagnostic code here as the problem is identified\n",
    "    book_author_clean = book_author.merge(\n",
    "        author_map.rename(columns={'author_id': 'db_author_id'}), # Rename to avoid conflicts\n",
    "        left_on=author_col,\n",
    "        right_on='name',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Select final columns and rename\n",
    "    book_author_clean = book_author_clean[['ISBN', 'db_author_id']].copy()\n",
    "    book_author_clean.rename(columns={'db_author_id': 'author_id'}, inplace=True)\n",
    "    \n",
    "    # Final data cleaning steps (deduplication and NaN removal)\n",
    "    book_author_clean.dropna(subset=['ISBN', 'author_id'], inplace=True)\n",
    "    book_author_clean.drop_duplicates(subset=['ISBN', 'author_id'], inplace=True)\n",
    "    \n",
    "    # Verify ISBNs exist in book table (FK check)\n",
    "    book_author_clean = book_author_clean[\n",
    "        book_author_clean['ISBN'].isin(existing_isbns['ISBN'])\n",
    "    ]\n",
    "    \n",
    "    book_author_clean['author_id'] = book_author_clean['author_id'].astype('Int64')\n",
    "    \n",
    "    # --- FIX FOR DuplicateColumnError ---\n",
    "    # Define the data types explicitly to prevent SQLAlchemy from creating conflicting schema\n",
    "    dtype_mapping = {\n",
    "        'ISBN': sqlalchemy.types.VARCHAR(13),  # Adjust size if your schema is different\n",
    "        'author_id': sqlalchemy.types.Integer \n",
    "    }\n",
    "    \n",
    "    # Final insertion\n",
    "    if len(book_author_clean) > 0:\n",
    "        book_author_clean.to_sql(\n",
    "            'book_author', \n",
    "            con=engine, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            dtype=dtype_mapping # <-- The fix\n",
    "        )\n",
    "        print(f\"Inserted {len(book_author_clean)} book-author relationships\")\n",
    "    else:\n",
    "        print(\"❌ Final DataFrame is empty. No rows inserted.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"book_author insertion failed: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "book_category_header",
   "metadata": {},
   "source": [
    "### 4.2 Book_Category Table - FIXED COLUMN MAPPING (FK to book and category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_book_category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 5964 book-category relationships\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    book_category = pd.read_csv(\"data/book_category.csv\")\n",
    "    \n",
    "    # Read the original category data to get the mapping from old ID to Name\n",
    "    category_csv = pd.read_csv(\"data/category.csv\")\n",
    "    \n",
    "    # Ensure the database map is clean\n",
    "    category_map['category_name'] = category_map['category_name'].astype(str).str.strip()\n",
    "    \n",
    "    # --- DIAGNOSTIC PASS (Retained) ---\n",
    "    # (The successful diagnostic code has been condensed for final insertion)\n",
    "    \n",
    "    if 'category_id' in book_category.columns:\n",
    "        old_cat_map = category_csv[['category_id', 'category_name']].drop_duplicates()\n",
    "        \n",
    "        # 1. Map old ID to Name\n",
    "        book_category_with_names = book_category.merge(\n",
    "            old_cat_map,\n",
    "            on='category_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        book_category_with_names['category_name'] = book_category_with_names['category_name'].astype(str).str.strip()\n",
    "        \n",
    "        # 2. Map Name to New DB ID\n",
    "        diagnostic_merge = book_category_with_names.merge(\n",
    "            category_map.rename(columns={'category_id': 'db_category_id'}),\n",
    "            on='category_name',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Final DataFrame preparation\n",
    "        book_category_clean = diagnostic_merge[['ISBN', 'db_category_id']].copy()\n",
    "        book_category_clean.rename(columns={'db_category_id': 'category_id'}, inplace=True)\n",
    "        book_category_clean['category_id'] = book_category_clean['category_id'].astype('Int64')\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Cannot find category_id column in book_category CSV\")\n",
    "    \n",
    "    book_category_clean.dropna(subset=['ISBN', 'category_id'], inplace=True)\n",
    "    book_category_clean.drop_duplicates(subset=['ISBN', 'category_id'], inplace=True)\n",
    "    \n",
    "    # Verify ISBNs exist\n",
    "    book_category_clean = book_category_clean[\n",
    "        book_category_clean['ISBN'].isin(existing_isbns['ISBN'])\n",
    "    ]\n",
    "    \n",
    "    # --- FIX FOR DuplicateColumnError ---\n",
    "    # Define the data types explicitly to prevent SQLAlchemy from creating conflicting schema\n",
    "    dtype_mapping = {\n",
    "        'ISBN': sqlalchemy.types.VARCHAR(13),  # Assuming ISBN is VARCHAR(13)\n",
    "        'category_id': sqlalchemy.types.Integer \n",
    "    }\n",
    "    \n",
    "    # Final insertion\n",
    "    if len(book_category_clean) > 0:\n",
    "        book_category_clean.to_sql(\n",
    "            'book_category', \n",
    "            con=engine, \n",
    "            if_exists='append', \n",
    "            index=False,\n",
    "            dtype=dtype_mapping # <-- The fix\n",
    "        )\n",
    "        print(f\"Inserted {len(book_category_clean)} book-category relationships\")\n",
    "    else:\n",
    "        print(\"Final DataFrame is empty. No rows inserted.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"book_category insertion failed: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5_header",
   "metadata": {},
   "source": [
    "## Step 5: Insert Book_Copy (FK to book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insert_book_copy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 5000 book copies\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # FIX: Get existing ISBNs fresh (in case previous cell didn't run or update the variable)\n",
    "    with engine.connect() as conn:\n",
    "        existing_isbns = pd.read_sql(\"SELECT ISBN FROM book\", conn)\n",
    "    \n",
    "    book_copy = pd.read_csv(\"data/book_copy.csv\")\n",
    "    book_copy_cols = ['status', 'condition', 'price', 'ISBN']\n",
    "    book_copy_clean = book_copy[book_copy_cols].copy()\n",
    "    \n",
    "    # Verify ISBNs exist (FK check)\n",
    "    book_copy_clean = book_copy_clean[\n",
    "        book_copy_clean['ISBN'].isin(existing_isbns['ISBN'])\n",
    "    ]\n",
    "    \n",
    "    book_copy_clean.dropna(subset=['ISBN'], inplace=True)\n",
    "    book_copy_clean.to_sql('book_copy', con=engine, if_exists='append', index=False)\n",
    "    print(f\"Inserted {len(book_copy_clean)} book copies\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"book_copy insertion failed: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_header",
   "metadata": {},
   "source": [
    "## Step 6: Insert Reservation and Reservation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reservation_header",
   "metadata": {},
   "source": [
    "### 6.1 Reservation Table - FIXED MAPPING (FK to member and staff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "insert_reservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 4503 reservations\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if len(member_map) == 0 or len(staff_map) == 0:\n",
    "        print(\"⚠️ Cannot insert reservations: No members or staff in database\")\n",
    "    else:\n",
    "        reservation = pd.read_csv(\"data/reservation.csv\")\n",
    "        member_csv = pd.read_csv(\"data/member.csv\")\n",
    "        staff_csv = pd.read_csv(\"data/staff.csv\")\n",
    "        \n",
    "        # Prepare mapping from old CSV IDs to Email (the shared unique identifier)\n",
    "        member_csv['phone'] = member_csv['phone'].astype(str).str[:20]\n",
    "        old_member_map = member_csv[['member_id', 'email']].drop_duplicates()\n",
    "        \n",
    "        staff_csv['phone'] = staff_csv['phone'].astype(str).str[:20]\n",
    "        old_staff_map = staff_csv[['staff_id', 'email']].drop_duplicates()\n",
    "        \n",
    "        # --- Member Mapping ---\n",
    "        reservation_with_email = reservation.merge(\n",
    "            old_member_map,\n",
    "            on='member_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Merge with actual database member IDs (member_map)\n",
    "        reservation_clean = reservation_with_email.merge(\n",
    "            member_map,\n",
    "            on='email',\n",
    "            how='inner',\n",
    "            suffixes=('_old', '_new')\n",
    "        )\n",
    "        \n",
    "        # --- Staff Mapping ---\n",
    "        # Use email as the bridge for staff mapping as well\n",
    "        reservation_clean = reservation_clean.merge(\n",
    "            old_staff_map.rename(columns={'staff_id': 'staff_id_old', 'email': 'staff_email'}),\n",
    "            left_on='staff_id',\n",
    "            right_on='staff_id_old',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        reservation_clean = reservation_clean.merge(\n",
    "            staff_map.rename(columns={'email': 'staff_email', 'staff_id': 'staff_id_new'}),\n",
    "            on='staff_email',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Select final columns\n",
    "        res_final = reservation_clean[[\n",
    "            'member_id_new', 'staff_id_new', 'reservation_date', \n",
    "            'expiration_date', 'returned_at'\n",
    "        ]].copy()\n",
    "        \n",
    "        res_final.rename(columns={\n",
    "            'member_id_new': 'member_id',\n",
    "            'staff_id_new': 'staff_id'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        res_final.to_sql('reservation', con=engine, if_exists='append', index=False)\n",
    "        print(f\"✅ Inserted {len(res_final)} reservations\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Reservation insertion failed: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "res_details_header",
   "metadata": {},
   "source": [
    "### 6.2 Reservation Details Table (FK to reservation and book_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "insert_res_details",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4503 reservation IDs from database\n",
      "✅ Inserted 9037 reservation details\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get new reservation IDs from database\n",
    "    with engine.connect() as conn:\n",
    "        db_reservations = pd.read_sql(\"SELECT reservation_id, member_id, reservation_date FROM reservation\", conn)\n",
    "    print(f\"Retrieved {len(db_reservations)} reservation IDs from database\")\n",
    "    \n",
    "    if len(db_reservations) == 0:\n",
    "        print(\"⚠️ No reservations in database, skipping reservation_details\")\n",
    "    else:\n",
    "        # Get book_copy IDs\n",
    "        with engine.connect() as conn:\n",
    "            copy_map = pd.read_sql(\"SELECT copy_id FROM book_copy\", conn)\n",
    "        \n",
    "        res_details = pd.read_csv(\"data/reservation_details.csv\")\n",
    "        \n",
    "        # ASSUMPTION: Map old reservation_id to new reservation_id by sequence/order\n",
    "        old_res_ids = sorted(res_details['reservation_id'].unique())\n",
    "        new_res_ids = db_reservations['reservation_id'].tolist()\n",
    "        \n",
    "        if len(old_res_ids) <= len(new_res_ids):\n",
    "            res_id_mapping = dict(zip(old_res_ids, new_res_ids[:len(old_res_ids)]))\n",
    "            \n",
    "            res_details_clean = res_details.copy()\n",
    "            res_details_clean['reservation_id'] = res_details_clean['reservation_id'].map(res_id_mapping)\n",
    "            \n",
    "            # Filter valid copy_ids and reservation_ids (FK checks)\n",
    "            res_details_clean = res_details_clean[\n",
    "                res_details_clean['copy_id'].isin(copy_map['copy_id']) &\n",
    "                res_details_clean['reservation_id'].notna()\n",
    "            ]\n",
    "            \n",
    "            res_details_clean.drop_duplicates(subset=['reservation_id', 'copy_id'], inplace=True)\n",
    "            \n",
    "            res_details_clean.to_sql(\n",
    "                'reservation_details', \n",
    "                con=engine, \n",
    "                if_exists='append', \n",
    "                index=False\n",
    "            )\n",
    "            print(f\"✅ Inserted {len(res_details_clean)} reservation details\")\n",
    "        else:\n",
    "            print(\"⚠️ Warning: More old reservations than new ones in database\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Reservation details insertion failed: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_header",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Let's verify the data was inserted correctly by counting the final row totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "verify_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Row Counts ---\n",
      "Total rows in description         : 2500\n",
      "Total rows in author              : 4460\n",
      "Total rows in category            : 802\n",
      "Total rows in member              : 3000\n",
      "Total rows in staff               : 100\n",
      "Total rows in book                : 2500\n",
      "Total rows in book_copy           : 5000\n",
      "Total rows in book_author         : 6450\n",
      "Total rows in book_category       : 5964\n",
      "Total rows in reservation         : 4503\n",
      "Total rows in reservation_details : 9037\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    tables = [\n",
    "        'description', 'author', 'category', 'member', 'staff', 'book', \n",
    "        'book_copy', 'book_author', 'book_category', 'reservation', 'reservation_details'\n",
    "    ]\n",
    "    print(\"--- Final Row Counts ---\")\n",
    "    for table in tables:\n",
    "        count = pd.read_sql(text(f\"SELECT COUNT(*) FROM {table}\"), conn).iloc[0, 0]\n",
    "        print(f\"Total rows in {table.ljust(20)}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
